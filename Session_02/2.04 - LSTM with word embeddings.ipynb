{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Session 2: Document Classification/Regression with Neural Networks</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 3: Long-Short Term Memory (LSTM) Networks with Word Embeddings as Features</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... understand the logic behind recurrent neural networks, especially LSTMs. and <br>\n","        ... are able to train a LSTM with word embeddings as features.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"Hr1lI3joctE6"},"source":["# What are Recurrent Neural Networks?"]},{"cell_type":"markdown","metadata":{"id":"lx6TSeClcvWb"},"source":["## Simple Recurrent Neural Network (RNN)\n","\n","A RNN processes sequences by iterating through the sequence elements and maintaining a *state* containing information relative to what it has seen so far. In effect, a RNN layer is a neural network layer with an internal loop, as shown in the figure below."]},{"cell_type":"markdown","metadata":{"id":"ZNlB5mtDdLzn"},"source":["<br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/rnn1.png\"/><br>\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"jOobrDD2dMBz"},"source":["The figure below shows a simple RNN unrolled over time. As can be seen from the figure the output of a layer is a combination of\n","\n","1. its direct data input (`input_t`),\n","2. the layer's state from the previous timestep (`state_t`), and\n","3. a bias term (`bo`)."]},{"cell_type":"markdown","metadata":{"id":"Qn6fuh8HdmTo"},"source":["<br><img width=700 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/rnn2.png\"/><br>\n","\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"xrYClhKAduA3"},"source":["## Long-Short Term Memory (LSTM) Networks"]},{"cell_type":"markdown","metadata":{"id":"TQny-Crkd0cJ"},"source":["Compared to a simple RNN layer, a LSTM layer contains one central innovation: A *carry track* that allows to carry over information over time from any previous timestep to the current timestep. Consequently, the output of a layer is a combination of\n","\n","\n","\n","1. its direct data input (`input_t`),\n","2. the layer's state from the previous timestep (`state_t`),\n","3. the input from the carry track (`c_t`), and\n","4. a bias term (`bo`)."]},{"cell_type":"markdown","metadata":{"id":"QlZU3v9xd_ia"},"source":["<br><img width=700 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/lstm.png\"/><br>\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"3VpSWKNzXDbv"},"source":["## Comparing RNNs with MLPs"]},{"cell_type":"markdown","metadata":{"id":"OmKWTO8DXTAq"},"source":["Andrej Karpathy's legendary blog post \"The Unreasonable Effectiveness of Recurrent Neural Networks\" contains a very informative comparison of RNNs with traditional neural networks: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `tensorflow` is an end-to-end open source platform for machine learning, especially deep learning.\n","- `matplotlib` is a plotting library for the Python programming language and its numerical mathematics extension NumPy\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from sklearn import metrics\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"0cT264vl78Zo"},"source":["Check if we are running on GPU."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"RHONHA667__1"},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.config.experimental.list_physical_devices('GPU')"]},{"cell_type":"markdown","metadata":{"id":"jZd82t53sqpu"},"source":["# Load documents"]},{"cell_type":"markdown","metadata":{},"source":["Load wine reviews (Source: https://www.kaggle.com/datasets/zynicide/wine-reviews) from a csv file."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["corpus = pd.read_csv(\"https://raw.githubusercontent.com/olivermueller/amlta-2024/main/Session_01/winemag-data-130k-v2.csv\")"]},{"cell_type":"markdown","metadata":{"id":"CureExnsIS-p"},"source":["Split data into three sets: training, validation, and test."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"BBpPCg5zILlu"},"outputs":[],"source":["training = corpus.iloc[0:80000,].sample(n=10000) # to speed up training\n","validation = corpus.iloc[80000:100000,]\n","test = corpus.iloc[100000:,]"]},{"cell_type":"markdown","metadata":{"id":"n5Mygm5yIYnh"},"source":["For each dataset, store features and targets in separate variables"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"18PF5d6ZIN-U"},"outputs":[],"source":["train_corpus_features = training[[\"description\"]]\n","train_corpus_target = training[[\"points\"]]\n","val_corpus_features = validation[[\"description\"]]\n","val_corpus_target = validation[[\"points\"]]\n","test_corpus_features = test[[\"description\"]]\n","test_corpus_target = test[[\"points\"]]"]},{"cell_type":"markdown","metadata":{"id":"gvhh11WuIeyk"},"source":["Create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas Dataframes. The use of TensorFlow Datasets follows a common pattern:\n","\n","1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n","2.   Apply transformations to preprocess the data in the dataset (e.g., vectorize text data).\n","3. Iterate over the dataset and process its elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n","\n","Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."]},{"cell_type":"code","execution_count":6,"metadata":{"id":"QOOszgPrQVVw"},"outputs":[],"source":["train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n","                                               tf.cast(train_corpus_target.values, tf.int32)))\n","\n","val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n","                                             tf.cast(val_corpus_target.values, tf.int32)))\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n","                                              tf.cast(test_corpus_target.values, tf.int32)))"]},{"cell_type":"markdown","metadata":{"id":"dXbjrq6NJ1Qk"},"source":["Display some stats and examples from the created datasets."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"j9LXPIY-XxDH"},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs.shape: (1,)\n","inputs.dtype: <dtype: 'string'>\n","targets.shape: (1,)\n","targets.dtype: <dtype: 'int32'>\n","===\n","inputs[0]: tf.Tensor(b\"Citrus blossom and leesy vanilla notes open this balanced aromatic white from Argentina's north. Leesy tropcial-fruit flavors focus on lychee and lime. The wine is moderately long on a simple but solid finish.\", shape=(), dtype=string)\n","targets[0]: tf.Tensor(89, shape=(), dtype=int32)\n"]}],"source":["for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"Uwh4TWJbVFY9"},"source":["# Vectorize documents"]},{"cell_type":"markdown","metadata":{"id":"rM7gEBL8KHzg"},"source":["We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors. Again, we map unique words to integers."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"oyJW-JetIN2w"},"outputs":[],"source":["max_tokens = 10000\n","max_length = 100\n","\n","text_vectorization = TextVectorization(\n","    max_tokens = max_tokens,\n","    output_mode = \"int\",\n","    output_sequence_length = max_length\n",")"]},{"cell_type":"markdown","metadata":{"id":"T4ElfvWtKj0Q"},"source":["Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function."]},{"cell_type":"code","execution_count":9,"metadata":{"id":"s6xCAUAOMdYH"},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-10-28 20:21:21.625387: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"]}],"source":["train_ds_features_only = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(train_ds_features_only)"]},{"cell_type":"markdown","metadata":{"id":"DkQu4KlNLftU"},"source":["Show the vocabulary that our vectorizer knows after being fit to the training data."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Dn0lt-7CRaCq"},"outputs":[{"data":{"text/plain":["['',\n"," '[UNK]',\n"," np.str_('and'),\n"," np.str_('the'),\n"," np.str_('a'),\n"," np.str_('of'),\n"," np.str_('with'),\n"," np.str_('this'),\n"," np.str_('is'),\n"," np.str_('wine')]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["text_vectorization.get_vocabulary()[0:10]"]},{"cell_type":"markdown","metadata":{"id":"VjeZpApaLvyx"},"source":["Next, we apply our `text_vectorization` function to all three datasets. This corresponds to step 2 mentioned above."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"jHGn2peqMYuP"},"outputs":[],"source":["vectorized_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)"]},{"cell_type":"markdown","metadata":{"id":"K_WzVu2WMEn-"},"source":["Show results."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"0etTA2eAUB6G"},"outputs":[{"name":"stdout","output_type":"stream","text":["inputs.shape: (1, 100)\n","inputs.dtype: <dtype: 'int64'>\n","targets.shape: (1,)\n","targets.dtype: <dtype: 'int32'>\n","===\n","inputs[0]: tf.Tensor(\n","[  59  462    2 1190   65   35  386    7   83  268   54   22 7156 1300\n"," 1190 7906   10  871   16  838    2  152    3    9    8  869  118   16\n","    4  184   23  194   20    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0], shape=(100,), dtype=int64)\n","targets[0]: tf.Tensor(89, shape=(), dtype=int32)\n"]}],"source":["for inputs, targets in vectorized_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"h9QH9PcrSa7J"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"G_K1l5L-MQ7z"},"source":["We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function).\n","\n","Instead of averaging or flattening the outputs of the embedding layer, a RNN/LSTM layer can directly process its 2D output (i.e., it takes a sequence of vectors as input instead of a single vector)."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"ZMB14gBYSblz"},"outputs":[],"source":["def get_model(hidden_dim=32):\n","    inputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n","    embedded = layers.Embedding(input_dim=max_tokens, output_dim=300, mask_zero=True)(inputs)\n","    hidden1 = layers.LSTM(hidden_dim, return_sequences = False)(embedded)\n","    outputs = layers.Dense(1, activation = \"linear\")(hidden1)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer = tf.optimizers.Adam(),\n","                  loss = \"mean_absolute_error\",\n","                  metrics = [\"mean_absolute_error\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"pWZtLzduT_sH"},"source":["Instantiate model and show it's architecture."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"02bcqmcvTIDW"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n","</pre>\n"],"text/plain":["\u001b[1mModel: \"functional\"\u001b[0m\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)  │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,000,000</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,624</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n","│                     │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n","</pre>\n"],"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n","│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n","│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m300\u001b[0m)  │  \u001b[38;5;34m3,000,000\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n","│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │     \u001b[38;5;34m42,624\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n","│                     │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n","├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │         \u001b[38;5;34m33\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n","└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,042,657</span> (11.61 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,042,657\u001b[0m (11.61 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,042,657</span> (11.61 MB)\n","</pre>\n"],"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,042,657\u001b[0m (11.61 MB)\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"],"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"]},"metadata":{},"output_type":"display_data"}],"source":["model = get_model()\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"0W4SzaRqUExq"},"source":["Fit model on training data and save best model to disk."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"aQZ8zbxbSzZb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/3\n","\u001b[1m 5848/10000\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 31ms/step - loss: 32.3825 - mean_absolute_error: 32.3825"]}],"source":["callbacks = [keras.callbacks.ModelCheckpoint(\"lstm.keras\", save_best_only=True)]\n","\n","history = model.fit(vectorized_train_ds.cache(),\n","          validation_data = vectorized_val_ds.cache(),\n","          epochs = 3,\n","          batch_size = 128,\n","          callbacks = callbacks)"]},{"cell_type":"markdown","metadata":{"id":"s7NOcSDY0Y6I"},"source":["Plot the learning process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BoHl0JaJ5fJm"},"outputs":[],"source":["plt.plot(history.history['mean_absolute_error'])\n","plt.plot(history.history['val_mean_absolute_error'])\n","plt.title('Model accuracy')\n","plt.ylabel('Mean Absolute Error')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wJ347rt_Tx0G"},"source":["# Make predictions"]},{"cell_type":"markdown","metadata":{"id":"XKR36ArkT2Gq"},"source":["Load best model from training phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA_ON4yBTb2a"},"outputs":[],"source":["model = keras.models.load_model(\"lstm.keras\")"]},{"cell_type":"markdown","metadata":{"id":"8LRhBvwPT1q-"},"source":["Make predictions on test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yckI-BTWSTFm"},"outputs":[],"source":["preds = model.predict(vectorized_test_ds)"]},{"cell_type":"markdown","metadata":{"id":"pMFi18uLT7tG"},"source":["Calculate accuracy metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afIwDkWYS_i4"},"outputs":[],"source":["print(metrics.mean_absolute_error(test_corpus_target, preds))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"amlta","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
