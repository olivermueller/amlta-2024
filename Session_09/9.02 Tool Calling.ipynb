{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAu-kSJo_wBW"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkW_65l0_wBX"
      },
      "source": [
        "# <font color=\"#003660\">Session 9: LLM-based Apps with LangChain</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2TLZqw4_wBX"
      },
      "source": [
        "# <font color=\"#003660\">Structured Outputs and Chains</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
        "        ... will know what tool calling is. <br>\n",
        "        ... will know how implement tools in LangChain and call tool chains.\n",
        "    </font>\n",
        "</div>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jCXZI28fEN0"
      },
      "source": [
        "The following content is heavily inspired by the following excellent sources:\n",
        "\n",
        "* [LangChain Academy](https://academy.langchain.com/)\n",
        "* [LangChain Docs (Python)](https://python.langchain.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb8FCaO3fEN0"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-community langchain-ollama ollama colab-xterm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "8G5Eit0y0uRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "THp68oAH0yHT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm # Copy this command in the Xterm starting below: HOST=0.0.0.0 ollama serve"
      ],
      "metadata": {
        "id": "sQF8pL9Q01cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull qwen2.5:1.5b"
      ],
      "metadata": {
        "id": "BY1Qk-9xTIQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ceIegjtfEN0"
      },
      "source": [
        "## Answering Maths Questions using LLMs\n",
        "\n",
        "Using computers we usually want calculations to be made of precision. Unfortunately, LLMs have problems with solving mathematical reasoning tasks as you can see in the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DDqf0AsEBqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from typing import Optional\n",
        "\n",
        "import torch\n",
        "from pydantic import BaseModel, Field\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_core.output_parsers import PydanticOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "from IPython.display import display, Latex, Markdown\n",
        "\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"mps:0\" if torch.mps.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sTx1W9OnjTr"
      },
      "outputs": [],
      "source": [
        "LLM_NAME = \"qwen2.5:1.5b\"\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=LLM_NAME,\n",
        "    temperature=0,\n",
        "    seed=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Also, what is 123 * 321?\"\n",
        "print(llm.invoke([{\"role\": \"user\", \"content\": query}]).content)"
      ],
      "metadata": {
        "id": "JLij-frBYxHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wow, that's wrong. The right answer is 123 * 321 = 39483."
      ],
      "metadata": {
        "id": "PBjs4QhP2i8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can we do abour this? Usually, a human would use a calculator. Can LLMs also do that?\n",
        "\n",
        "This is the concept of Tool-LLMs ([Qin et al. (2023)](https://doi.org/10.48550/arXiv.2307.16789)) or [function calling](https://www.promptingguide.ai/applications/function_calling).\n",
        "\n",
        "*Function calling or tool usage* such as ToolAlpaca [Tang et al. (2023)](https://doi.org/10.48550/arXiv.2306.05301) allows to define functions or tools that the llm can use during execution.\n",
        "\n",
        "First we have to define a `tool`, like the `add` and `multiply` functions below."
      ],
      "metadata": {
        "id": "5vnAxPbl2qO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\"\"\"\n",
        "    return a * b"
      ],
      "metadata": {
        "id": "KmMAcx1QHTIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to bind these tools to the llm using LangChains `.bind_tools`."
      ],
      "metadata": {
        "id": "HOodccvl4pHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [add, multiply]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "NJReqIGz4odB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it."
      ],
      "metadata": {
        "id": "tG6bLoCK4yMx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4o7TduFfEN1"
      },
      "outputs": [],
      "source": [
        "query = \"Also, what is 123 * 321?\" # correct answer: 39483\n",
        "\n",
        "print(llm_with_tools.invoke(query).model_dump_json(indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see in the output, the LLM extracts the `\"tool_calls\"` with\n",
        "\n",
        "```\n",
        "{\n",
        "    \"name\": \"multiply\",\n",
        "    \"args\": {\n",
        "        \"a\": 123,\n",
        "        \"b\": 321\n",
        "    },\n",
        "    \"id\": \"f98fe5d8-dd6a-4a67-938a-bc020fc27d15\",\n",
        "    \"type\": \"tool_call\"\n",
        "}\n",
        "```\n",
        "\n",
        "a is 123 and b is 321, that is right.\n",
        "But how can we really use this tool and how can we get the tool answer back."
      ],
      "metadata": {
        "id": "9qUgE_1p42D0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to store the answer of out ToolLLM. This can be done using the code below."
      ],
      "metadata": {
        "id": "YMtXKjXG5OsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"Also, what is 123 * 321?\"\n",
        "\n",
        "messages = [HumanMessage(query)]\n",
        "\n",
        "ai_msg = llm_with_tools.invoke(messages)\n",
        "\n",
        "messages.append(ai_msg)\n",
        "\n",
        "print(ai_msg.tool_calls)"
      ],
      "metadata": {
        "id": "l85H4x05UyhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we can iterate over the list of tool calls and run them while appending the results to the messages as in the code below."
      ],
      "metadata": {
        "id": "RjAnwWOj5Xwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tool_call in ai_msg.tool_calls:\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "    tool_msg = selected_tool.invoke(tool_call)\n",
        "    messages.append(tool_msg)\n",
        "    print(tool_call)\n",
        "    print(tool_msg.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "etvBDmlpcDI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use all tool answers in the `messages`we can then prompt the LLM and get our final answer."
      ],
      "metadata": {
        "id": "ZnM_FjLN5s_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_with_tools.invoke(messages).content)"
      ],
      "metadata": {
        "id": "OPeizoUnc5TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your Task"
      ],
      "metadata": {
        "id": "7inUJqKfn-91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LLM_NAME = \"qwen2.5:1.5b\"\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=LLM_NAME,\n",
        "    temperature=0,\n",
        "    seed=0\n",
        ")"
      ],
      "metadata": {
        "id": "Wi9K9OpPoBhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is 222053/899?\"\n",
        "display(Markdown(llm.invoke([{\"role\": \"user\", \"content\": query}]).content.replace(\"\\\\[\", \"$\").replace(\"\\\\]\", \"$\").replace(\"\\\\(\", \"$\").replace(\"\\\\)\", \"$\")))"
      ],
      "metadata": {
        "id": "lU_wx8vIiXiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This answer seems to be wrong. The answer should be 247.\n",
        "\n",
        "Can you extend the code to get a correct answer?\n",
        "\n",
        "**Hint:** You will have to add a new tool to the code below."
      ],
      "metadata": {
        "id": "xr3z7HSH57LA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tool\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Adds a and b.\"\"\"\n",
        "    return a + b\n",
        "\n",
        "@tool\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiplies a and b.\"\"\"\n",
        "    return a * b\n",
        "\n",
        "# add a new tool here\n"
      ],
      "metadata": {
        "id": "AxtDitocmj_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# maybe you have to change something here\n",
        "tools = [add, multiply]\n",
        "\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "_N5d1zIe6YK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_with_tools.invoke(query).model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "y09NnQzAnvBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [HumanMessage(query)]\n",
        "\n",
        "ai_msg = llm_with_tools.invoke(messages)\n",
        "\n",
        "messages.append(ai_msg)\n",
        "\n",
        "print(ai_msg.tool_calls)"
      ],
      "metadata": {
        "id": "UGmQK3-Rh5wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and some changes here?\n",
        "for tool_call in ai_msg.tool_calls:\n",
        "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
        "    tool_msg = selected_tool.invoke(tool_call)\n",
        "    messages.append(tool_msg)\n",
        "    print(tool_call)\n",
        "    print(tool_msg.model_dump_json(indent=4))"
      ],
      "metadata": {
        "id": "VcREOEIDh-g9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_with_tools.invoke(messages).content)"
      ],
      "metadata": {
        "id": "mazRe0gziK_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aml4ta",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}