{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAu-kSJo_wBW"
      },
      "source": [
        "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkW_65l0_wBX"
      },
      "source": [
        "# <font color=\"#003660\">Session 8: Retrieval-Augmented Generation</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2TLZqw4_wBX"
      },
      "source": [
        "# <font color=\"#003660\">RAG Basics</font>\n",
        "\n",
        "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
        "\n",
        "<p>\n",
        "\n",
        "<div>\n",
        "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
        "        ... will know the basics of Retrieval-Augmented Generation (RAG) is. <br>\n",
        "        ... will know to implement a RAG-chain from scratch.\n",
        "    </font>\n",
        "</div>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following content is heavily inspired by the following excellent sources:\n",
        "\n",
        "* [HuggingFace (2024): NLP Course](https://huggingface.co/learn/nlp-course/)\n",
        "* [Huggingface (2024): Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/index)\n",
        "* [Nguyen (2024): Code a simple RAG from scratch](https://huggingface.co/blog/ngxson/make-your-own-rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -U pymupdf4llm datasets transformers faiss-cpu accelerate langchain langchain-community langchain-huggingface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answering Questions using LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3DDqf0AsEBqn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import pymupdf4llm\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, set_seed\n",
        "\n",
        "DEVICE = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sTx1W9OnjTr"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=DEVICE,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(messages):    \n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tp8XWUAGoNNC",
        "outputId": "45a7242b-b0e6-444d-a859-decf59910b1a"
      },
      "outputs": [],
      "source": [
        "set_seed(0)\n",
        "prompt = \"Who plays Daenerys Targaryen?\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "]\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "WOW! How wrong this answer is! (The actress is Emilia Clarke.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To go on we need to restart the runtime to free the GPU memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieval-Augmented Generation\n",
        "![](https://github.com/olivermueller/amlta-2024/blob/main/Session_08/imgs/RAG.png?raw=true)\n",
        "\n",
        "(Image adapted from [Kaltenpoth and Müller (2024)](https://energy.acm.org/eir/dont-touch-the-power-line-a-proof-of-concept-for-aligned-llm-based-assistance-systems-to-support-the-maintenance-in-the-electricity-distribution-system/))\n",
        "\n",
        "Retrieval-augmented generation (RAG), introduced by [Lewis et al. (2020)](https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html) incorporates external knowledge in form of a vector database into the language model answers. A retriever (mostly an encoder-only transformer lm) retrieves k documents most similar to the query. Those documents provide the context for an LLM to answer the user question.\n",
        "\n",
        "Now let's check if it improves the answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing a basic RAG system from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import pymupdf4llm\n",
        "import urllib\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM, set_seed\n",
        "\n",
        "DEVICE = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.mkdir(\"documents\")\n",
        "os.mkdir(\"imgs\")\n",
        "os.mkdir(\"markdown_documents\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/olivermueller/amlta-2024/refs/heads/main/Session_08/documents/Game_of_Thrones.pdf\", \"documents/Game_of_Thrones.pdf\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/olivermueller/amlta-2024/refs/heads/main/Session_08/documents/How_I_Met_Your_Mother.pdf\", \"documents/How_I_Met_Your_Mother.pdf\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/olivermueller/amlta-2024/refs/heads/main/Session_08/markdown_documents/Game_of_Thrones.md\", \"markdown_documents/Game_of_Thrones.md\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/olivermueller/amlta-2024/refs/heads/main/Session_08/markdown_documents/How_I_Met_Your_Mother.md\", \"markdown_documents/How_I_Met_Your_Mother.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HD_958XVBNn6"
      },
      "source": [
        "# Processing PDFs\n",
        "\n",
        "We will provide two PDF files: A Wikipedia entry of the TV series Game of Thrones and another entry of the How I met your Mother TV series to show that our retrieval works and it is not just luck.\n",
        "As PDFs are not directly readable for LLMs, we need to convert them in a readable format such as markdown. For that we can use [PyMuPDF4LLM](https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/) a library sxplicitly designed to convert PDFs to LLM-readable markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VuFfbkDCB24j"
      },
      "outputs": [],
      "source": [
        "documents_path = \"documents\"\n",
        "markdown_documents_path = \"markdown_documents\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tw5Tqo5FDpQd",
        "outputId": "57dce638-424c-4106-afd9-26ec6b7a9877"
      },
      "outputs": [],
      "source": [
        "documents = os.listdir(documents_path)\n",
        "\n",
        "for document in documents:\n",
        "    document_path = os.path.join(documents_path, document)\n",
        "    md_file = pymupdf4llm.to_markdown(\n",
        "        document_path\n",
        "    )\n",
        "    md_file_path = os.path.join(markdown_documents_path, document.replace(\".pdf\", \".md\"))\n",
        "    with open(md_file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(md_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Designing a Vector Storage (Retrieval Database)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we need to design the vector storage based on our documents.\n",
        "\n",
        "![](https://github.com/olivermueller/amlta-2024/blob/main/Session_08/imgs/vectordb.png?raw=true)\n",
        "\n",
        "(Adapted from [Xie et al. (2023)](https://doi.org/10.1109/BigDIA60676.2023.10429609))\n",
        "\n",
        "A vector database for retrieval is splitted into indexing and querying. While indexing is done using an encoder once, the querying is done via similarity search, e.g., cosine similarity ([Xie et al., 2023](https://doi.org/10.1109/BigDIA60676.2023.10429609))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_documents = os.listdir(markdown_documents_path)\n",
        "\n",
        "md_files = []\n",
        "\n",
        "for markdown_document in markdown_documents:\n",
        "    markdown_document_path = os.path.join(markdown_documents_path, markdown_document)\n",
        "    with open(markdown_document_path) as file:\n",
        "        md_files.append([markdown_document, file.read()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(md_files[0][1][:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(md_files[0][1][:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see in the displayed markdown above, there are many cross-references within the wikipedia articles that will disturb the LLM. Therefore, we will remove the markdown links (with little help of ChatGPT)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_markdown_links(text):\n",
        "    \"\"\"\n",
        "    Removes Markdown links from the given text while keeping the link text.\n",
        "    \n",
        "    Args:\n",
        "        text (str): The input Markdown text.\n",
        "        \n",
        "    Returns:\n",
        "        str: The text with Markdown links removed. \n",
        "    \n",
        "    Yeah this was ChatGPT ;)\n",
        "    \"\"\"\n",
        "    # Regex to match Markdown links [text](link)\n",
        "    pattern = r'\\[([^\\]]+)\\]\\([^\\)]+\\)'\n",
        "    # Replace the matched pattern with just the text inside the brackets\n",
        "    cleaned_text = re.sub(pattern, r'\\1', text)\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(Markdown(remove_markdown_links(md_files[0][1])[:1000]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(remove_markdown_links(md_files[0][1])[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now the text looks more readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "markdown_documents = os.listdir(markdown_documents_path)\n",
        "\n",
        "md_files = []\n",
        "\n",
        "for markdown_document in markdown_documents:\n",
        "    markdown_document_path = os.path.join(markdown_documents_path, markdown_document)\n",
        "    with open(markdown_document_path) as file:\n",
        "        md_files.append([markdown_document, remove_markdown_links(file.read())])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0HDJx72BNn6"
      },
      "source": [
        "### Chunking Texts\n",
        "\n",
        "As usually neither encoder nor decoder models can hold complete documents in their contexts, the documents are usually chunked.\n",
        "\n",
        "We will use some chunking according to token length. It is also common to use some overlap between the chunks to represent how the contained information chain semantically ([Wang et al., 2024](https://doi.org/10.18653/v1/2024.emnlp-main.981))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OAx_bv8IoUR",
        "outputId": "ff083b26-8e01-4d06-8052-62098abd469e"
      },
      "outputs": [],
      "source": [
        "embedding_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\", model_max_length=999999999)\n",
        "print(\"File name:\", md_files[0][0], \"Tokens:\", len(tokenizer(md_files[0][1], truncation=False)[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GWTRZjerJ-pn"
      },
      "outputs": [],
      "source": [
        "OVERLAP = 32\n",
        "CHUNK_LENGTH = 512\n",
        "\n",
        "md_files_chunked = []\n",
        "for md_file in md_files:\n",
        "    md_file_tokenized = embedding_tokenizer(md_file[1], truncation=False)[\"input_ids\"]\n",
        "    for i in range(CHUNK_LENGTH, len(md_file_tokenized), CHUNK_LENGTH-OVERLAP):\n",
        "        md_files_chunked.append([md_file[0], embedding_tokenizer.decode(md_file_tokenized[i-CHUNK_LENGTH: i], skip_special_tokens=True)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oYT7i37MDPD",
        "outputId": "b99ec540-656d-46ea-f161-d7e31ace8c3a"
      },
      "outputs": [],
      "source": [
        "print(md_files_chunked[0])\n",
        "print(md_files_chunked[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVTflp3QBNn6"
      },
      "source": [
        "### Loading Embedding Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJYAdE0dNsBB",
        "outputId": "ab421b7f-d03f-4a93-b9ec-2acc70c61076"
      },
      "outputs": [],
      "source": [
        "embedding_tokenizer = AutoTokenizer.from_pretrained(\"jinaai/jina-embeddings-v2-base-en\")\n",
        "embedding_model = AutoModel.from_pretrained('jinaai/jina-embeddings-v2-base-en', trust_remote_code=True)\n",
        "embedding_model.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tMs9m7dJORyZ"
      },
      "outputs": [],
      "source": [
        "embeddings = embedding_model.encode([md_files_chunked[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68kz1zwPOoI3",
        "outputId": "fdf6d6a9-f92d-4d68-805a-e8eefe119eb2"
      },
      "outputs": [],
      "source": [
        "print(list(embeddings[0][:10]) + [\"...\"], len(embeddings[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JXtkfs3BNn6"
      },
      "source": [
        "### Creating the final vector storage (Retrieval Database)\n",
        "\n",
        "We will apply the easiest (and slowest) way of storing the vectors in a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "02312a152f55445dae86003ee71f4659",
            "3b561c32d4d140c5b4d258afada93396",
            "2b64e5a9433c4e94aae678e8913615d4",
            "62f53d04f3634a03b776fdbbfc084a4d",
            "b05dc476ef9240ca9b936caf63ab3727",
            "cf923a66c44545a0b4986300c6850962",
            "ac09e1d0f1404f5aa03be4c0414b6fd7",
            "27544349234c41e1ac3ce0484ae6ab16",
            "c86e08eb90cf4653a7bef03b0fda9f03",
            "b14a9dedce904d5c8c9eac2f5319c171",
            "2cff53cb86ed41c287b97c7c0359b761"
          ]
        },
        "id": "ibxL_mfgR4pr",
        "outputId": "ef8bdccc-34fe-48bc-8b87-e753954d7e34"
      },
      "outputs": [],
      "source": [
        "VECTOR_DB = []\n",
        "\n",
        "for md_file in tqdm(md_files_chunked):\n",
        "    VECTOR_DB.append({\"embeddings\": embedding_model.encode([md_file[1]])[0], \"content\": md_file[1], \"metadata\":{\"source\": md_file[0]}})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zc7XQf3VUMJO"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(a, b):\n",
        "  dot_product = sum([x * y for x, y in zip(a, b)])\n",
        "  norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
        "  norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
        "  return dot_product / (norm_a * norm_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "zz1ZafAMUTOh"
      },
      "outputs": [],
      "source": [
        "def retrieve(query, top_n=3, embedding_model=embedding_model):\n",
        "  query_embedding = embedding_model.encode([query])[0]\n",
        "  # temporary list to store (chunk, similarity) pairs\n",
        "  similarities = []\n",
        "  # TODO: calculate cosine similarity between query and each chunk in the VECTOR_DB\n",
        "  # Hint: each VECTOR_DB entry is a dictionary with keys \"embeddings\" (embeddings) and \"content\" (text chunk)\n",
        "\n",
        "\n",
        "\n",
        "  #write your code here\n",
        "  \n",
        "  \n",
        "  \n",
        "  # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
        "  similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "  # finally, return the top N most relevant chunks\n",
        "  return similarities[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xp8tBgbRf-GS",
        "outputId": "993a27c9-2112-4f95-e736-81a9699f2308"
      },
      "outputs": [],
      "source": [
        "retrieved_docs = retrieve(\"Who plays Daenerys Targaryen?\")\n",
        "\n",
        "for doc in retrieved_docs:\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eacnU9hpBNn7"
      },
      "source": [
        "## Determining the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "62H_dGGRrYY8"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=DEVICE,\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=DEVICE,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Defining a Generation Prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = '''Use only the following context chunks to answer the question: {input_query}\n",
        "Don't make up any new information. Here are the chunks:\n",
        "{chunks}\n",
        "Now anwser the question: {input_query}'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANLlGJp2BNn7"
      },
      "source": [
        "# Generating based on Documents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So now let's bring everythin together. We can reuse the ``generate_response`` method for querying an LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(messages):    \n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=512\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DyIlcAZlXdS",
        "outputId": "eab3972c-0418-481a-e734-13316f6056ab"
      },
      "outputs": [],
      "source": [
        "input_query = \"Who plays Daenerys Targaryen?\"\n",
        "retrieved_knowledge = retrieve(input_query)\n",
        "\n",
        "print('Retrieved knowledge:')\n",
        "for chunk, similarity in retrieved_knowledge:\n",
        "    print(f' - (similarity: {similarity:.2f}) {chunk}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "chunks = '\\n'.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])\n",
        "question = prompt.format(input_query=input_query, chunks=chunks, input_query=input_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4ZWh6IumkIN",
        "outputId": "fb25f461-ed73-40ea-9391-0b49fd27031e"
      },
      "outputs": [],
      "source": [
        "set_seed(0) # for reproducibility\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": question}\n",
        "]\n",
        "response = generate_response(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rnaw87S3BNn7"
      },
      "source": [
        "# Creating a RAG Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your TODO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Of53nqeBmsDK"
      },
      "outputs": [],
      "source": [
        "def generate_rag_response(input_query):\n",
        "    # TODO: retrieve relevant chunks from the VECTOR_DB\n",
        "    # TODO: create a prompt using the retrieved chunks and the input query\n",
        "    # TODO: generate a response using the RAG model\n",
        "    # TODO: return the generated response\n",
        "\n",
        "\n",
        "    # write your code here\n",
        "\n",
        "\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R895sCarBNn7"
      },
      "source": [
        "# Finally a RAG Chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV1mT03wmmPJ"
      },
      "outputs": [],
      "source": [
        "set_seed(0)\n",
        "input_query = input('Ask me a question: ')\n",
        "print(generate_rag_response(input_query))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://imgflip.com/i/9fsgxc\"><img src=\"https://i.imgflip.com/9fsgxc.jpg\" title=\"made at imgflip.com\"/></a><div><a href=\"https://imgflip.com/memegenerator\">from Imgflip Meme Generator</a></div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aml4ta",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02312a152f55445dae86003ee71f4659": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3b561c32d4d140c5b4d258afada93396",
              "IPY_MODEL_2b64e5a9433c4e94aae678e8913615d4",
              "IPY_MODEL_62f53d04f3634a03b776fdbbfc084a4d"
            ],
            "layout": "IPY_MODEL_b05dc476ef9240ca9b936caf63ab3727"
          }
        },
        "27544349234c41e1ac3ce0484ae6ab16": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b64e5a9433c4e94aae678e8913615d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27544349234c41e1ac3ce0484ae6ab16",
            "max": 443,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c86e08eb90cf4653a7bef03b0fda9f03",
            "value": 443
          }
        },
        "2cff53cb86ed41c287b97c7c0359b761": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b561c32d4d140c5b4d258afada93396": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf923a66c44545a0b4986300c6850962",
            "placeholder": "​",
            "style": "IPY_MODEL_ac09e1d0f1404f5aa03be4c0414b6fd7",
            "value": "100%"
          }
        },
        "62f53d04f3634a03b776fdbbfc084a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b14a9dedce904d5c8c9eac2f5319c171",
            "placeholder": "​",
            "style": "IPY_MODEL_2cff53cb86ed41c287b97c7c0359b761",
            "value": " 443/443 [00:07&lt;00:00, 61.93it/s]"
          }
        },
        "ac09e1d0f1404f5aa03be4c0414b6fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05dc476ef9240ca9b936caf63ab3727": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b14a9dedce904d5c8c9eac2f5319c171": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86e08eb90cf4653a7bef03b0fda9f03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf923a66c44545a0b4986300c6850962": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
