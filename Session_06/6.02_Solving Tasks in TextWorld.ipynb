{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Session 6: Solving Tasks with Prompting LLMs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Notebook 2: Solving Tasks in TextWorld</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
    "        ... you know what TextWorld is and how it works, <br>\n",
    "        ... will learn about system prompts and meta prompting <br>\n",
    "        ... will learn how to apply prompting to \"automate\" a process.\n",
    "    </font>\n",
    "</div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following content is heavily inspired by the following excellent sources:\n",
    "\n",
    "\n",
    "* [Raschka (2024): Building a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "* [HuggingFace (2024): NLP Course](https://huggingface.co/learn/nlp-course/)\n",
    "* [Huggingface (2024): Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/index)\n",
    "* [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "* [Ollama](https://github.com/ollama/ollama)\n",
    "* [TextWorld Documentation](https://textworld.readthedocs.io/en/stable/notes/framework.html)\n",
    "* [Microsoft Research Blog](https://www.microsoft.com/en-us/research/blog/first-textworld-problems-the-competition-using-text-based-games-to-advance-capabilities-of-ai-agents/)\n",
    "* [TextWorld GitHub](https://github.com/microsoft/TextWorld)\n",
    "* [Côté et al. (2018)](https://doi.org/10.48550/arXiv.1806.11532)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextWorld and the First TextWorld Problems Challenge\n",
    "\n",
    "TextWorld was a learning environment for text-based games, specifically designed by Microsoft to support environments for text-based reinforcement agents ([Côté et al., 2018](https://doi.org/10.48550/arXiv.1806.11532)). With the publication of TextWorld, Microsoft also published the *First TextWorld Problems Challenge* which included *The Cooking Game*, where text-based agents need to navigate through different rooms, find the recipe in the kitchen, find, cut, cook the ingredients according to the recipe and eat it, using pre-defined text-commands on a diverse set of ingredients ([Microsoft, 2019](https://www.microsoft.com/en-us/research/blog/first-textworld-problems-the-competition-using-text-based-games-to-advance-capabilities-of-ai-agents/)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://www.microsoft.com/en-us/research/project/textworld/try-it/', width=1024, height=768)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the FTWP Challenge and its Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textworld termcolor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "from glob import glob\n",
    "from typing import Mapping, Any\n",
    "from termcolor import colored\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import textworld\n",
    "from textworld import EnvInfos\n",
    "import textworld.gym\n",
    "from textworld.gym import Agent\n",
    "\n",
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def secure_listdir(path, rm_dirs=[\".ipynb_checkpoints\", ]):\n",
    "    files = os.listdir(path)\n",
    "    for rm_dir in rm_dirs:\n",
    "        if rm_dir in files:\n",
    "            files.remove(rm_dir)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HumanAgent(Agent):\n",
    "    \"\"\" Agent that randomly selects a command from the admissible ones. \"\"\"\n",
    "    def __init__(self, seed=42):\n",
    "        self.seed = seed\n",
    "        self.messages = []\n",
    "        self.messages_raw = []\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return EnvInfos()\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        action = input()\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(agent, path, max_step=50, seed=42, verbose=True, render=True):\n",
    "    random.seed(seed)  # For reproducibility when using action sampling.\n",
    "    np.random.seed(seed)  # For reproducibility when using action sampling.\n",
    "\n",
    "    infos_to_request = agent.infos_to_request\n",
    "    infos_to_request.max_score = True  # Needed to normalize the scores.\n",
    "\n",
    "    gamefiles = [path]\n",
    "    if os.path.isdir(path):\n",
    "        gamefiles = glob(os.path.join(path, \"*.z8\"))\n",
    "\n",
    "    env_id = textworld.gym.register_games(\n",
    "        gamefiles,\n",
    "        request_infos=infos_to_request,\n",
    "        max_episode_steps=max_step\n",
    "    )\n",
    "    \n",
    "    env = textworld.gym.make(env_id)  # Create a Gym environment to play the text game.\n",
    "    if verbose:\n",
    "        if os.path.isdir(path):\n",
    "            print(os.path.dirname(path))\n",
    "        else:\n",
    "            print(os.path.basename(path))\n",
    "    \n",
    "    obs, infos = env.reset()  # Start new episode.\n",
    "    if render:\n",
    "        env.render()  # Print the initial observation.\n",
    "\n",
    "    obs_template_violation = \"Your answer was not conform to the template provided. Provide the answer according to your taks description.\"\n",
    "\n",
    "    score = 0\n",
    "    done = False\n",
    "    moves = 0\n",
    "    while not done:\n",
    "        command = agent.act(obs, score, done, infos)\n",
    "        \n",
    "        rethink_counter = 0\n",
    "        while len(command) > 50:\n",
    "            command = agent.act(obs_template_violation, score, done, infos)\n",
    "            rethink_counter += 1\n",
    "            moves += 1\n",
    "            if rethink_counter == 3:\n",
    "                break\n",
    "        if len(command) > 50:\n",
    "            command = \"I don't know\"\n",
    "        \n",
    "        obs, score, done, infos = env.step(command)\n",
    "        if render:\n",
    "            env.render()  # Print the command's feedback.\n",
    "        moves += 1\n",
    "\n",
    "    agent.act(obs, score, done, infos)  # Let the agent know the game is done.\n",
    "\n",
    "    norm_score = score / infos[\"max_score\"]\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    if verbose:\n",
    "        msg = \"Moves: {:3d}\\tScore: {:4d} / {:4d}\\tNormalized Score: {:3.2f}\"\n",
    "        print(msg.format(moves, score, infos[\"max_score\"], norm_score))\n",
    "    \n",
    "    return moves, score, norm_score, infos[\"max_score\"], agent.messages, agent.messages_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMAgent(Agent):\n",
    "    \"\"\" LLM agent. \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm, \n",
    "            prompt,\n",
    "            history_length, \n",
    "            system_message,\n",
    "            examples, \n",
    "            env_infos={}, \n",
    "            seed=42,\n",
    "            render=False,\n",
    "            options={\"seed\": 42},\n",
    "        ):\n",
    "        self.llm = llm\n",
    "        self.prompt = prompt\n",
    "        self.history_length = history_length\n",
    "        self.messages = [\n",
    "            {\n",
    "                'role': 'system', \n",
    "                'content': system_message, \n",
    "            }, \n",
    "        ]\n",
    "        self.examples = examples\n",
    "        self.messages += self.examples\n",
    "        self.env_infos = EnvInfos(**env_infos)\n",
    "        self.seed = seed\n",
    "        self.render = render\n",
    "        self.recipe = \"You have not read the recipe yet.\"\n",
    "        self.messages_raw = []\n",
    "        self.options=options\n",
    "\n",
    "    @property\n",
    "    def infos_to_request(self) -> EnvInfos:\n",
    "        return self.env_infos\n",
    "\n",
    "    def act(self, obs: str, score: int, done: bool, infos: Mapping[str, Any]) -> str:\n",
    "        inventory = infos.get(\"inventory\", \"\").strip()\n",
    "        description = infos.get(\"description\", \"\").strip()\n",
    "        feedback = obs.strip()\n",
    "\n",
    "        if 'You open the copy of \"Cooking: A Modern Approach (3rd Ed.)\" and start reading:' in feedback:\n",
    "            self.recipe = feedback.replace('You open the copy of \"Cooking: A Modern Approach (3rd Ed.)\" and start reading:', \"\").strip()\n",
    "\n",
    "        prompt = self.prompt.format(feedback=feedback, recipe=self.recipe, inventory=inventory, description=description)\n",
    "        self.messages_raw.append({'role': 'user', \"feedback\": feedback, \"recipe\": self.recipe, \"inventory\": inventory, \"description\": description})\n",
    "        self.messages.append({'role': 'user', 'content': prompt})\n",
    "        if self.render:\n",
    "            print(prompt)\n",
    "        \n",
    "        context_messages = self.messages[0:len(self.examples) + 1] + self.messages[1:][-min(self.history_length * 2 + 1, len(self.messages) - (len(self.examples) + 1)):]\n",
    "\n",
    "        answer = client.chat(model=self.llm, messages=context_messages, options=self.options).get(\"message\", {}).get(\"content\", \"\")\n",
    "        self.messages.append({'role': 'assistant', 'content': answer})\n",
    "        self.messages_raw.append({'role': 'assistant', 'content': answer})\n",
    "        if self.render:\n",
    "            print(colored(\"> \" + answer, \"yellow\"))\n",
    "        answer = answer.split(\":\")[-1].strip()\n",
    "        \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, train_files, train_path, RENDER_AGENT, RENDER, RESULTS_PATH, OPTIONS):\n",
    "    score_sum = 0\n",
    "    max_score_sum = 0\n",
    "    wins = 0\n",
    "    train_results = {}\n",
    "    for train_file in tqdm(train_files):\n",
    "        train_file_path = os.path.join(train_path, train_file)\n",
    "        config_file_path = train_file_path.replace(\".z8\", \".json\")\n",
    "        config = {}\n",
    "        with open(config_file_path) as config_file:\n",
    "            config = json.load(config_file)\n",
    "        \n",
    "        walkthrough = config.get(\"extras\", {}).get(\"walkthrough\", [])\n",
    "        min_moves = len(walkthrough)\n",
    "        recipe = config.get(\"extras\", {}).get(\"recipe\", \"\")\n",
    "        \n",
    "        moves, score, norm_score, max_score, messages, messages_raw = play(LLMAgent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, 42, render=RENDER_AGENT, options=OPTIONS), train_file_path, render=RENDER)\n",
    "        train_results[train_file] = {\"moves\": moves, \"min_moves\": min_moves, \"score\": score, \"norm_score\": norm_score, \"max_score\": max_score, \"recipe\": recipe, \"walkthrough\":walkthrough, \"messages\": messages, \"messages_raw\": messages_raw}\n",
    "        with open(RESULTS_PATH, \"w\") as f:\n",
    "            json.dump(train_results, f)\n",
    "        score_sum += score\n",
    "        max_score_sum += max_score\n",
    "        if score == max_score:\n",
    "            wins += 1\n",
    "    print(f\"Average normalized score: {score_sum / max_score_sum}\")\n",
    "    print(f\"Win rate: {wins / len(train_files)}\")\n",
    "    print(f\"Overall wins: {wins}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with a small subset of the FTWP challenge, to ensure the feasibility of the game with prompting.\n",
    "\n",
    "The agent:\n",
    "* starts in the kitchen\n",
    "* needs to:\n",
    "    - read the recipe\n",
    "    - find all ingredients in the kitchen (maybe open the fridge)\n",
    "    - take all ingredients and the knife\n",
    "    - cut all ingredients according to the recipe\n",
    "    - cook all ingredients according to the recipe\n",
    "    - prepare the meal\n",
    "    - eat the meal\n",
    "* gets the following information in every round:\n",
    "    - the game engines feedback\n",
    "    - the recipe (if it has read it)\n",
    "    - the inventory\n",
    "    - the description of the current room\n",
    "\n",
    "Let's try this out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(HumanAgent(), \"games/train/tw-cooking-recipe1-6yMNiKXmIgPjhepy.z8\", render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's finish the environmental setup and start prompting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "OPTIONS = {\n",
    "    \"seed\": RANDOM_SEED,\n",
    "    \"num_ctx\": 8192,\n",
    "    \"num_predict\": -2,\n",
    "}\n",
    "\n",
    "proteus_host='http://131.234.154.103:11434'\n",
    "client = Client(host=proteus_host)\n",
    "\n",
    "games_dir = \"games\"\n",
    "\n",
    "train_split = \"train\"\n",
    "train_path = os.path.join(games_dir, train_split)\n",
    "train_files = secure_listdir(train_path)\n",
    "train_files = [x for x in train_files if x.endswith(\".z8\")]\n",
    "\n",
    "valid_split = \"valid\"\n",
    "valid_path = os.path.join(games_dir, valid_split)\n",
    "valid_files = secure_listdir(valid_path)\n",
    "valid_files = [x for x in valid_files if x.endswith(\".z8\")]\n",
    "\n",
    "HISTORY_LENGTH = 3\n",
    "RENDER_AGENT = True # renders processed the agent's view\n",
    "RENDER = False # renders the unprocessed game view\n",
    "MODEL_NAME = \"gemma2\" # 9 billion parameter model\n",
    "\n",
    "ENV_INFOS = {\n",
    "    \"description\": True,\n",
    "    \"inventory\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompts and Meta Prompting\n",
    "\n",
    "*System prompt* is a common phrase describing general basic instruction and a general behavior the LLM conversational agent should adhere to ([Li et al., 2024](https://doi.org/10.48550/arXiv.2402.10962)). \n",
    "\n",
    "A *meta prompt*, in contrast, is an example-agnostic structured prompt that captures the reasoning structure of a specific task category ([Zhang et al., 2024](https://doi.org/10.48550/arXiv.2311.11482))\n",
    "\n",
    "While these two have distinct definition, [Zhang et al. (2024)](https://doi.org/10.48550/arXiv.2311.11482) define also structured system meta prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![System and Meta Prompt](imgs/sys_meta.png)\n",
    "\n",
    "([Zhang et al., 2024](https://doi.org/10.48550/arXiv.2311.11482))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's adapt this to our cooking task in TextWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES = { # you can enter example messages for one-shot or few-shot prompting here\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"You are the player of a text-based cooking game.\n",
    "\n",
    "These are the important game entities: \n",
    "cookbook:       Cookbook that contains the recipe.\n",
    "<ingredient>:   Ingredient you can take, drop, cut, or cook according to the recipe.\n",
    "<container>:    Container you can open, which can contain ingredients.\n",
    "<door>:         Door you can open.\n",
    "knife:          Knife to slice, dice, chop ingredients.\n",
    "stove:          Stove to fry ingredients.\n",
    "oven:           Oven to roast ingredients.\n",
    "bbq:            BBQ to grill ingredients.\n",
    "meal:           Meal you can eat.\n",
    "\n",
    "These are the commands to interact with the game:\n",
    "look\n",
    "open <container>\n",
    "close <container>\n",
    "open <door>\n",
    "close <door>\n",
    "inventory\n",
    "take <ingredient>\n",
    "take knife\n",
    "read cookbook\n",
    "slice <ingredient> with knife\n",
    "dice <ingredient> with knife\n",
    "chop <ingredient> with knife\n",
    "cook <ingredient> with stove    (fry an ingredient)\n",
    "cook <ingredient> with oven     (roast an ingredient)\n",
    "cook <ingredient> with bbq      (grill an ingredient)\n",
    "prepare meal\n",
    "eat meal\n",
    "\n",
    "This is the basic game process:\n",
    "Navigate to the kitchen and read the cookbook.\n",
    "Take all ingredients according to the recipe.\n",
    "Follow the recipe.\n",
    "Eat the meal.\n",
    "\n",
    "Your main task:\n",
    "You provide answers to game engine queries following the command templates and game process above.\n",
    "Never provide answers deviating from the command templates above - unless explicitly asked by the game engine.\n",
    "Provide only one command at a time - unless the game engine explicitly asks to do something else.\"\"\"\n",
    "\n",
    "PROMPT = \"\"\"# Current game state:\n",
    "## {feedback}\n",
    "\n",
    "## {recipe}\n",
    "\n",
    "## {inventory}\n",
    "\n",
    "## You are in the {description}\"\"\"\n",
    "\n",
    "\n",
    "STRATEGY_NAME = \"BASIC\"\n",
    "RESULTS_PATH = os.path.join(\"results\", train_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, train_files[0:1], train_path, RENDER_AGENT, RENDER, RESULTS_PATH, OPTIONS) # evaluate on a single game to show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, train_files, train_path, False, RENDER, RESULTS_PATH, OPTIONS) # skip rendering during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add CoT prompting to this and check out, if we can improve the preformance.\n",
    "\n",
    "Additionally this concept is called *Instruction-style* prompting ([Zhou et al., 2023](https://doi.org/10.18653/v1/2023.findings-emnlp.968)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Reason step-by-step about the next command based on the current game state using the format REASONING: <your reasoning here> NEXT COMMAND: <your next command here>\n",
    "# Current game state:\n",
    "## {feedback}\n",
    "\n",
    "## {recipe}\n",
    "\n",
    "## {inventory}\n",
    "\n",
    "## You are in the {description}\n",
    "\n",
    "REASONING:\n",
    "\"\"\" # CoT part of the prompt\n",
    "\n",
    "STRATEGY_NAME = \"CoT\"\n",
    "RESULTS_PATH = os.path.join(\"results\", train_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, train_files, train_path, False, RENDER, RESULTS_PATH, OPTIONS) # skip rendering during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improved only by 2 wins. Let's check if this improvement is \"real\" on our validation dataset. (The validation dataset is not our test dataset.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation (distinct from Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"# Current game state:\n",
    "## {feedback}\n",
    "\n",
    "## {recipe}\n",
    "\n",
    "## {inventory}\n",
    "\n",
    "## You are in the {description}\"\"\"\n",
    "\n",
    "\n",
    "STRATEGY_NAME = \"BASIC\"\n",
    "RESULTS_PATH = os.path.join(\"results\", valid_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, valid_files, valid_path, False, RENDER, RESULTS_PATH, OPTIONS) # skip rendering during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\"Reason step-by-step about the next command based on the current game state using the format REASONING: <your reasoning here> NEXT COMMAND: <your next command here>\n",
    "# Current game state:\n",
    "## {feedback}\n",
    "\n",
    "## {recipe}\n",
    "\n",
    "## {inventory}\n",
    "\n",
    "## You are in the {description}\n",
    "\n",
    "REASONING:\n",
    "\"\"\" # CoT part of the prompt\n",
    "\n",
    "STRATEGY_NAME = \"CoT\"\n",
    "RESULTS_PATH = os.path.join(\"results\", valid_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, valid_files, valid_path, False, RENDER, RESULTS_PATH, OPTIONS) # skip rendering during evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you have learned so far:\n",
    "* *System prompts* can be used to align a model generally on a task.\n",
    "* *Meta prompts* are distinct from system prompts and are basically providing a structure for reasoning in LLMs.\n",
    "* However, meta prompts can also be used in system prompting.\n",
    "* Multiple prompting methods can be combined to improve results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Turn!\n",
    "\n",
    "Now you can try to define and change prompts by yourself.\n",
    "\n",
    "If you need some inspiration, refer to:\n",
    "* [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "* [The Prompt Report](https://doi.org/10.48550/arXiv.2406.06608)\n",
    "* [OpenWebUI Prompts](https://openwebui.com/prompts/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Trainig (Prompt Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "EXAMPLES = {\n",
    "\n",
    "}\n",
    "\n",
    "PROMPT = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "\n",
    "STRATEGY_NAME = \"my_magic\"\n",
    "RESULTS_PATH = os.path.join(\"results\", train_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, train_files, train_path, RENDER_AGENT, RENDER, RESULTS_PATH, OPTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "EXAMPLES = {\n",
    "\n",
    "}\n",
    "\n",
    "PROMPT = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "\n",
    "STRATEGY_NAME = \"my_magic\"\n",
    "RESULTS_PATH = os.path.join(\"results\", valid_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, valid_files, valid_split, RENDER_AGENT, RENDER, RESULTS_PATH, OPTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Already 20 Wins in the Valid Set?\n",
    "\n",
    "Try to evaluate with the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split = \"test\"\n",
    "test_path = os.path.join(games_dir, test_split)\n",
    "test_files = secure_listdir(test_path)\n",
    "test_files = [x for x in test_files if x.endswith(\".z8\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "EXAMPLES = {\n",
    "\n",
    "}\n",
    "\n",
    "PROMPT = \"\"\"<your magic here>\"\"\"\n",
    "\n",
    "\n",
    "STRATEGY_NAME = \"my_magic\"\n",
    "RESULTS_PATH = os.path.join(\"results\", test_split, f\"{STRATEGY_NAME}.json\")\n",
    "\n",
    "evaluate_agent(MODEL_NAME, PROMPT, HISTORY_LENGTH, SYSTEM_MESSAGE, EXAMPLES, ENV_INFOS, test_files, test_split, RENDER_AGENT, RENDER, RESULTS_PATH, OPTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session_06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
