{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Session 6: Solving Tasks with Prompting LLMs</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Notebook 1: Getting Familar with Ollama</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n",
    "        ... you know how to use Ollama for prompting LLMs, <br>\n",
    "        ... how to use its generate and chat api.\n",
    "    </font>\n",
    "</div>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following content is heavily inspired by the following excellent sources:\n",
    "\n",
    "\n",
    "* [Raschka (2024): Building a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\n",
    "* [HuggingFace (2024): NLP Course](https://huggingface.co/learn/nlp-course/)\n",
    "* [Huggingface (2024): Open-Source AI Cookbook](https://huggingface.co/learn/cookbook/index)\n",
    "* [Prompt Engineering Guide](https://www.promptingguide.ai/)\n",
    "* [Ollama](https://github.com/ollama/ollama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama? What is that? Why aren't we using HuggingFace?\n",
    "\n",
    "The answer is simple: We will use **Large** Langage Models (LLMs) - They are **large**.\n",
    "\n",
    "Therefore, we will use ollama, which is a simple package running LLMs and making them accessible via API. --> This allows us all to use really large LMs concurrently.\n",
    "\n",
    "In HuggingFace you can simply use a [HuggingFace Pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to setup a conda environment.\n",
    "Open the terminal, type the following command and hit enter.\n",
    "```\n",
    "conda create -n session_06 python=3.11\n",
    "```\n",
    "You will be asked to proceed in the terminal. Answer with \"y\" and hit ENTER.\n",
    "\n",
    "```\n",
    "conda activate session_06\n",
    "```\n",
    "You can copy the commands below (don't copy the hashtag)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command creates a new conda environment called session_06 including Python 3.11, JupyterLab, and ipywidgets.\n",
    "# conda create -n session_06 python=3.11 jupyterlab ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Visual Studio Code select the python environment \"session_06\" as kernel for the notebook:\n",
    "<div>\n",
    "<img src=\"https://code.visualstudio.com/assets/docs/datascience/jupyter-kernel-management/noterbook-kernel-picker.gif\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "If not, run the command ``jupyter lab`` in your opened terminal. You browser usually will open automatically and provide the jupyter lab environment.\n",
    "\n",
    "After that, run the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Completion with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will use the python implementation of [Ollama](https://ollama.com/).\n",
    "\n",
    "Ollama is a package that allows to create, manage, modify and prompt LLMs using a pre-implemented API architecture. --> No API coding, yeah!\n",
    "\n",
    "If you want to learn more about Ollama refer to the [Ollama Website](https://ollama.com/) and the [Ollama GitHub](https://github.com/ollama/ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a Client für connecting to one of the sodalab computers, which is already running ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host='http://131.234.154.103:11434'\n",
    "client = Client(host=host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client can list all models as well as create, delete, modify LLMs.\n",
    "The client also allows to generate and chat using LLMs.\n",
    "\n",
    "Now let's list all models that are available in our ollama instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = client.list()\n",
    "print(model_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is many different information provided for every model.\n",
    "We only want to know the names for loading a model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [x[\"model\"] for x in model_list[\"models\"]]\n",
    "for name in model_names:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the ``\"gemma2:27b-text-q4_0\"`` model, which is a mid-size LLM developed by Google and can run in inference mode on our sodalab computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:27b-text-q4_0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating text with Gemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama ``.generate`` uses a specific ``model`` to complete text based on an input prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"We will, we will rock you\\nWe will, we will rock you\\n\"\n",
    "response = client.generate(\n",
    "    model=model_name, \n",
    "    prompt=prompt,\n",
    "    options={\"seed\": 42, \"num_predict\": 50} # standard temperature is 0.7\n",
    ")\n",
    "response[\"context\"] = response[\"context\"][:3] + [\"...\"] + [response[\"context\"][-1]] # ignore that, it is only for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response object contains the actual text ``\"response\"``as well as other information, such as ``\"prompt_eval_count\"`` (number of prompt tokens), ``\"eval_count\"`` (number of answer tokens),  ``\"context\"`` (tokenized prompt and answer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\033[93mJSON response:\\033[0m\")\n",
    "print(response)\n",
    "print()\n",
    "print(\"\\033[93mOur input prompt:\\033[0m\")\n",
    "print(prompt)\n",
    "print(\"\\033[93mGenerated text:\\033[0m\")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add all options available in the [model API](https://github.com/ollama/ollama/blob/main/docs/modelfile.md#valid-parameters-and-values), such as ``num_predict`` regulating the number of tokens to predict, ``temperature`` of the model, or the ``seed`` for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"We will, we will rock you\\nWe will, we will rock you\\n\"\n",
    "response = client.generate(\n",
    "    model=model_name, \n",
    "    prompt=prompt,\n",
    "    options={\"seed\": 42, \"num_predict\": 50, \"temperature\": 0.0}\n",
    ")\n",
    "print(\"\\033[93mGenerated text:\\033[0m\")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"We will, we will rock you\\nWe will, we will rock you\\n\"\n",
    "response = client.generate(\n",
    "    model=model_name, \n",
    "    prompt=prompt,\n",
    "    options={\"seed\": 42, \"num_predict\": 50, \"temperature\": 3}\n",
    ")\n",
    "print(\"\\033[93mGenerated text:\\033[0m\")\n",
    "print(response[\"response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompting Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompting VS Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prompting** can be referred to as passing downstream tasks as textual prompts, unambiguous instructions reformulated to solve like the training data, to LLMs without further retraining while **Prompt (template) engineering** denotes the development of the most appropriate prompt to solve a task ([Kaltenpoth and Müller, 2024](https://aisel.aisnet.org/wi2024/91/); [Liu et al., 2023](https://doi.org/10.1145/3560815))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the process of prompt engineering.\n",
    "First, let's write a function that generates a response using the ``options``, and ``model_name`` we have already defined and only receives a prompt as input.\n",
    "Second, we write another function that prints the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model_name=model_name, options={\"seed\": 42, \"num_predict\": 50}):\n",
    "    response = client.generate(\n",
    "        model=model_name, \n",
    "        prompt=prompt,\n",
    "        options=options\n",
    "    )\n",
    "    return response[\"response\"]\n",
    "\n",
    "def print_response(prompt, model_name=model_name, options={\"seed\": 42, \"num_predict\": 50}):\n",
    "    print(\"\\033[93mPrompt:\\033[0m\")\n",
    "    print(prompt)\n",
    "    response = generate_response(prompt, model_name, options)    \n",
    "    print(\"\\033[93mGenerated text:\\033[0m\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What did Thomas Edison invent?\"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the answer is not helpful at all. Using the recommendations of [Liu et al. (2023)](https://doi.org/10.1145/3560815) we reformulate it as written in a text describing Alber Einstein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Thomas Edison invented \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, it's working. Let's try another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"In which year ended the second World War?\"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... the model completes a possible exam question, but we wanted an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Q: In which year ended the second World War?\\nA: \" # provide Q: A: format\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example are maths questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is 25/4? \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Q: What is 25/4?\\nA: \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Zero-Shot Prompting to Few-Shot Prompting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have done so far is called *zero-shot prompting* ([Liu et al., 2023](https://doi.org/10.1145/3560815);[Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)). Prompting for an answer directly. This mostly works in easy cases but not with more complex tasks.\n",
    "\n",
    "Let's look at an example, where we want to get the sentiment of a very short movie \"review\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"'I love this movie' is a \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not helpful. Now lets try to give the model an example, which es referred to as *one-shot prompting* ([Brown et al., 2020](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"'I hate this movie' is a negative comment.\\n'I love this movie' is a\"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complex math problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is 11x4/2?\"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not helpful. Let's try it with a one-shot example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is 7x5? 35 What is 11x4/2? \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not working, too. Let's use *few-shot prompting*, which is simply using (one or) more examples ([Liu et al., 2023](https://doi.org/10.1145/3560815);[Brown et al., 2020](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is 7x8? 56 What is 9x9? 81 What is 7x5? 35 What is 11x4/2? \"\n",
    "print_response(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, it worked. So let's briefly summarize what we learned:\n",
    "\n",
    "* *Promping* refers to passing (unambiguously defined) natural language instructions to an LLM.\n",
    "* *Prompt engineering* refers to defining the most appropriate prompt for a task.\n",
    "* *Zero-shot prompting is just prompting the model without examples.\n",
    "* *Few-Shot (One-shot) prompting* referst to prompting the model with one or more examples of the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As most of you will know ChatGPT, what is different in this models answer compared to those of ChatGPT?\n",
    "\n",
    "Write it as commend below and share your answer with the seminar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write here your answer to the questions: What is different in this models answer compared to those of ChatGPT?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Foundation Models VS Chat (Instruction) Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Foundation Models VS Chat Models](imgs/llms.png)\n",
    "\n",
    "(Image adapted from ([Raschka (2024)](https://www.manning.com/books/build-a-large-language-model-from-scratch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible in the image above, a foundation model is generated by *pretraining*, as those model you trained in the last session.\n",
    "\n",
    "After pretraining a model, you can fine-tune it to follow instructions or in a conversational manner ([Ouyang et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf)).\n",
    "\n",
    "This is what was done with ChatGPT, Claude and other models ([OpenAI, 2022](https://openai.com/index/chatgpt/);[Ganguli et al., 2022](https://doi.org/10.48550/arXiv.2209.07858))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatting with Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup the chat API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just load the imports and client again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proteus_host='http://131.234.154.103:11434'\n",
    "client = Client(host=proteus_host)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama provides the ``/chat/`` endpoint for chatting with instruction or conversational models.\n",
    "\n",
    "We also need to load another version of the gemma model: ``gemma2:27b``, which refers to the 27 billion parameter instruction following model version of [Gemma2](https://ollama.com/library/gemma2:27b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:27b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Ollama python, the ``.chat`` needs messages in the format provided below, which is similar to the OpenAI API format.\n",
    "\n",
    "The messages need to be a list of dictionaries that contain a 'role' and 'content' of type string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"How are you?\",\n",
    "    },\n",
    "]\n",
    "response = client.chat(model=model_name, messages=messages, options={\"seed\": 42, \"num_predict\": 50})\n",
    "print(\"Ollama respones:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ollama response contains diverse information on the processing, besides the responses ``\"message\"`` and its ``\"content\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ollama response message:\")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write two functions for simplification again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chat_response(messages, model_name=model_name, options={\"seed\": 42, \"num_predict\": 50}):\n",
    "    response = client.chat(\n",
    "        messages=messages,\n",
    "        model=model_name, \n",
    "        options=options\n",
    "    )\n",
    "    return response['message']['content']\n",
    "\n",
    "def print_chat_response(messages, model_name=model_name, options={\"seed\": 42, \"num_predict\": 50}):\n",
    "    print(\"\\033[93mConversation history:\\033[0m\")\n",
    "    for message in messages:\n",
    "        print(f\"{message['role']}: {message['content']}\")\n",
    "    response = generate_chat_response(messages, model_name, options)    \n",
    "    print(\"\\033[93mAnswer:\\033[0m\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The messages list needs to start with a message with the ``\"system\"`` role (e.g, \"You are a helpful AI assistant that answers questions.\") or a ``\"user\"`` role.\n",
    "\n",
    "The system message or *system prompt* mostly contains general instructions that preceed task specific details ([Zhang et al., 2024](https://doi.org/10.48550/arXiv.2410.14826))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are an AI chatbot giving sarcastic answers.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"How are you?\",\n",
    "    },\n",
    "]\n",
    "print_chat_response(messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a message history, the messages should to be alternating ``\"user\"`` and ``\"assistant\"``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are an AI chatbot giving sarcastic answers.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"How are you?\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'assistant',\n",
    "        'content': \"I looooove being an AI chatbot! Always being asked the same questions over and over again. It's the best!\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': \"What do you love the most?\",\n",
    "    },\n",
    "]\n",
    "print_chat_response(messages=messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought Prompting and Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's think of a more complex math riddle, probably for five graders ([Williams and Huckle, 2024](https://doi.org/10.48550/arXiv.2405.19616)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_message = \"Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have?\"\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful and honest AI chatbot that follows user instructions and answers questions honestly and helpfully.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': my_message,\n",
    "    },\n",
    "]\n",
    "print_chat_response(messages=messages, options={\"seed\": 42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model answers wrong, while thinking the right way.\n",
    "\n",
    "The problem is that language models predict the next token by the probability of the previous context ([Shanahan et al., 2024](https://doi.org/10.1038/s41586-023-06647-8)).\n",
    "\n",
    "This can lead to higher probabilities for the wrong tokens if they are learned from shortcut answers in the training data.\n",
    "\n",
    "To prevent this, [Wei et al. (2022)](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf) introduced *Chain-of-Thought (CoT) prompting*, which asks the model to reason step-by-step before answering the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Chain-of-Thought Prompting](imgs/cot.png)\n",
    "\n",
    "\n",
    "As visible in this illustraion from the Paper of Wei et al. (2022), you can see that they apply few-shot prompting to incorporate the CoT reasoning into the model ([Wei et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current models have already be trained for CoT reasoning. Therefore, we only need to instruct the model to think step-by-step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_message = \"Sally (a girl) has 3 brothers. Each brother has 2 sisters. How many sisters does Sally have? Please think step-by-step.\"\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful and honest AI chatbot that follows user instructions and answers questions honestly and helpfully.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': my_message,\n",
    "    },\n",
    "]\n",
    "print_chat_response(messages=messages, options={\"seed\": 42})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the answer is right now. Amazing!\n",
    "\n",
    "So lets think about another example.\n",
    "\n",
    "``\"When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\"``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    import torch\n",
    "    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, set_seed\n",
    "\n",
    "    set_seed(42)\n",
    "    model_path = \"google/gemma-2-2b-it\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    quantization_config = nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    ) # bitsandbytes only support specific CPU models\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path, \n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes CoT Prompting just leads to wrong answers, as in this example below.\n",
    "\n",
    "Please just look at the outputs and don't remove the ``if False:``, as the code can't run on your computer.\n",
    "For this example you need to install [PyTorch](https://pytorch.org/get-started/locally/), as well as transformers and bitsandbytes. (We will learn more about this libraries in the next session.)\n",
    "\n",
    "Then *self-consistency* is a helpful approach of prompting ([Wang et al. 2022](https://doi.org/10.48550/arXiv.2203.11171))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "    set_seed(0)\n",
    "    my_message = \"When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': my_message,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    model_inputs = tokenizer(chat_prompt, return_tensors='pt').to(\"cuda\") # this won't work on CPU\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "print(\"\"\"user\n",
    "When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\n",
    "* Step 1:  Find your sister's age when you were 6. \n",
    "* Step 2: Find the difference in your ages.\n",
    "* Step 3:  Add that difference to your current age. \n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "* **Step 1:** You were 6 years old, and your sister was half your age, so she was 6 / 2 = 3 years old.\n",
    "\n",
    "* **Step 2:** The difference in your ages is 6 - 3 = 3 years.\n",
    "\n",
    "* **Step 3:**  Add that difference to your current age: 70 + 3 = 73 years old. \n",
    "\n",
    "**Answer:** Your sister is 73 years old. \n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-consistency uses CoT prompting with a more diverse decoding strategy, that replaces *greedy* decoding with a majority vote as visible in the provided in the paper of ([Wang et al. (2022)](https://doi.org/10.48550/arXiv.2203.11171)).\n",
    "\n",
    "![Self-consistency](imgs/selfcon.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this with our example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "    set_seed(0)\n",
    "    my_message = \"When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': my_message,\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    chat_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    model_inputs = tokenizer(chat_prompt, return_tensors='pt').to(\"cuda\") # this won't work on CPU\n",
    "\n",
    "    beam_outputs = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7, # same as ollama\n",
    "        do_sample=True,\n",
    "        num_return_sequences=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:    \n",
    "    for beam_output in beam_outputs:\n",
    "        print(tokenizer.decode(beam_output, skip_special_tokens=True))\n",
    "        print(\"-\" * 25)\n",
    "        print(\"-\" * 25)\n",
    "print(\"\"\"user\n",
    "When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\n",
    "* Think about how old you were when you were 6.  \n",
    "* Think about how much older your sister is than you.  \n",
    "* Do these things to figure out how old she is now. \n",
    "\n",
    "\n",
    "**Here's the breakdown:**\n",
    "\n",
    "* **Step 1:** When you were 6, your sister was half your age, meaning she was 6 / 2 = 3 years old. \n",
    "* **Step 2:**  This means your sister is 3 years younger than you.\n",
    "* **Step 3:** You are now 70 years old.\n",
    "* **Step 4:** To find your sister's age, subtract the age difference from your current age, which is 70 - 3 = 67.\n",
    "\n",
    "**Answer:** Your sister is 67 years old. \n",
    "\n",
    "-------------------------\n",
    "-------------------------\n",
    "user\n",
    "When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\n",
    "*Remember the question asks for your sister's age, not your age*\n",
    "\n",
    "**Step 1:** When you were 6, your sister was half your age, which means she was 6 / 2 = 3 years old.\n",
    "\n",
    "**Step 2:**  The age difference between you and your sister is 6 - 3 = 3 years.\n",
    "\n",
    "**Step 3:** Now you are 70 years old.\n",
    "\n",
    "**Step 4:** Since the age difference remains the same, your sister is 70 - 3 = 67 years old.\n",
    "\n",
    "\n",
    "**Answer:** Your sister is 67 years old. \n",
    "\n",
    "-------------------------\n",
    "-------------------------\n",
    "user\n",
    "When I was 6 my sister was half my age. Now I’m 70 how old is my sister? Please think step-by-step.\n",
    "* When you were 6, your sister was half your age. \n",
    "* Therefore, your sister was 6/2 = 3 years old.\n",
    "* Now you are 70 years old. \n",
    "* The age difference between you and your sister is 70-6 = 6 years.\n",
    "* Therefore, your sister is 70-6 = 64 years old.\n",
    "\n",
    "\n",
    "\n",
    "**Answer:** Your sister is 64 years old. \n",
    "\n",
    "-------------------------\n",
    "-------------------------\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is one wrong answer, but the majority vote would be \"67 years\", which is right. Using this decoding strategy with a tree-like structure results in *Tree-of-Thoughts (ToT)* ([Yao et al., 2023](https://doi.org/10.48550/arXiv.2305.10601)), which is out of scope in most cases due to its heavy computation requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can run this cells again!\n",
    "\n",
    "While we can work with majority votings, we can also change the reasoning that is used for generating answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:2b\"\n",
    "my_message = \"India is larger than Russia.\"\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful and honest AI chatbot that follows user instructions and answers questions honestly and helpfully.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': my_message,\n",
    "    },\n",
    "]\n",
    "print_chat_response(model_name=model_name, messages=messages, options={\"seed\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the answer is wrong, while the reasoning (after the zero-shot answer) is right.\n",
    "\n",
    "So let's adress this by first generating some knowledge using a one-shot prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:2b\"\n",
    "my_message = \"\"\"\n",
    "Input: Glasses always fog up.\n",
    "Knowledge: Condensation occurs on eyeglass lenses when water vapor from your sweat, breath, and ambient humidity lands on a cold surface, cools, and then changes into tiny drops of liquid, forming a film that you see as fog. Your lenses will be relatively cool compared to your breath, especially when the outside air is cold.\n",
    "Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Input: India is larger than Russia.\n",
    "Knowledge:\n",
    "\"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful and honest AI chatbot that follows user instructions and answers questions honestly and helpfully. You just provide knowledge.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': my_message,\n",
    "    },\n",
    "]\n",
    "response = generate_chat_response(messages=messages, model_name=model_name, options={\"seed\": 0}).strip()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this as an input, we now use one-shot prompting to gather an answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gemma2:2b\"\n",
    "my_message = f\"\"\"\n",
    "Input: Greece is larger than mexico.\n",
    "Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n",
    "Answer: Greece is smaller than Mexico.\n",
    "Hypothesis: India is larger than Russia.\n",
    "Knowledge: {response}\n",
    "Answer: \"\"\"\n",
    "messages = [\n",
    "    {\n",
    "        'role': 'system',\n",
    "        'content': \"You are a helpful and honest AI chatbot that follows user instructions and answers questions honestly and helpfully.\",\n",
    "    },\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': my_message,\n",
    "    },\n",
    "]\n",
    "print_chat_response(model_name=model_name, messages=messages, options={\"seed\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, the model answers right, based on the knowledge.\n",
    "\n",
    "This is called *generated knowledge prompting* invented by [Liu et al. (2022)](https://doi.org/10.48550/arXiv.2110.08387)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize:\n",
    "\n",
    "You just learned:\n",
    "* Zero-shot prompting often fails in terms of correctness ([Radford et al., 2019](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)).\n",
    "* One-shot or few-shot prompting can improve this in easy settings  ([Liu et al., 2023](https://doi.org/10.1145/3560815);[Brown et al., 2020](https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)).\n",
    "* Chain-of-Thought (CoT) prompting can improve this in more complex settings ([Wei et al., 2022](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf)).\n",
    "* Self-consistency, Tree-of-Thoughts or generated knowledge prompting can further improve model responses ([Liu et al., 2022](https://doi.org/10.48550/arXiv.2110.08387);[Wang et al., 2022](https://doi.org/10.48550/arXiv.2203.11171);[Yao et al., 2023](https://doi.org/10.48550/arXiv.2305.10601)).\n",
    "\n",
    "But most of the Chain-of-Thought improvements are given to mathematical or true-false problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is: What happens when we want to solve real(-world) tasks? (As you may remember, the topic of this session is \"Solving Tasks with prompting LLMs\".)\n",
    "\n",
    "How can we use these prompting techniques, how do we use system prompts? How to reason within a conversation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "session_06",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
